<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="EUREQA">
  <meta name="keywords" content="EUREQA, QA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="EUREQA" />
	<meta property="og:type" content="website" />
	<meta property="og:url" content="https://vincentleebang.github.io/eureqa.github.io/" />
	<meta property="og:image" content="images/bathTub.png" />
  <title>EUREQA</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/galaxy.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EUREQA</h1>
            <h3 class="title is-3 publication-title">Are LLMs following the correct reasoning paths?</h3>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://vincentleebang.github.io" style="color:#022851;font-weight:normal;">Bangzheng Li</a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#f68946;font-weight:normal;">Ben Zhou</a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#008AD7;font-weight:normal;">Fei Wang</a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#f68946;font-weight:normal;">Xingyu Fu</a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#f68946;font-weight:normal;">Dan Roth</a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#022851;font-weight:normal;">Muhao Chen</a>,
              </span>
            </div>

            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#022851; font-weight:normal">&#x25B6 </b>University of California, Davis</b></span>
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b>University of Pennsylvania</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&nbsp&nbsp&#x25B6 </b> University of Southern California</span>
              <!-- <span class="author-block">&nbsp&nbsp<sup>*</sup>Equal Contribution</span> -->
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2311.09702.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/VincentLeebang/eureqa" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/vincentleebang/EUREQA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
           We propose a novel probing method and benchmark called EUREQA. EUREQA is an entity-searching task where a model finds a missing entity based on described multi-hop relations with other entities. These deliberately designed multi-hop relations create deceptive semantic associations, and models must stick to the correct reasoning path instead of incorrect shortcuts to find the correct answer.

           Experiments show that existing LLMs cannot follow correct reasoning paths and resist the attempt of greedy shortcuts. Analyses provide further evidence that LLMs rely on semantic biases to solve the task instead of proper reasoning, questioning the validity and generalizability of current LLMs’ high performances.
        </h4>
      </div>
    </div>
  </section>

  <section class="section"  >
    <div class="container is-max-desktop" id="fig1">

      <figure style="text-align: center;">
        <img id="teaser" width="100%" src="images/fig1.png">  
        <figcaption>
          LLMs make errors when correct surface-level semantic cues-entities are recursively replaced with descriptions, and the errors are likely related to token similarity. GPT-3.5-turbo is used for this example.
        </figcaption>
      </figure>
<!--  -->
    </div>
  </section>



  
<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="images/datasetIcon.png"> The EUREQA dataset</h2>
      <span style="font-size: 95%;">Download the dataset from <a href="https://huggingface.co/datasets/vincentleebang/EUREQA">[Dataset]</a></span>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified">
        <p>

          In EUREQA, every question is constructed through an implicit reasoning chain. The chain is constructed by parsing DBPedia and each layer of the chain comprises three components: an entity, a fact about the entity, and a relation between the entity 
          and its counterpart from the next layer. The layers stack up to create chains with diffrent depths of reasoning. We verbolize reasoning chains into natural sentences and anonymize the entity of each layer to create the question. 
          Questions can be solved layer by layer and each layer is guaranteed to have a unique answer. EUREQA is not a knowledge game: we adopt a knowledge filtering process which ensures  that most LLMs have sufficient world knowledge to answer our questions.
          <br>
          EUREQA comprises a total of 2,991 questions of different reasoning depths and difficulties. The entities encompass a broad spectrum of topics, effectively reducing any potential bias arising from specific entity categories. 
          These data are great for analyzing the reasoning processes of LLMs
          <!-- <span style="font-variant: small-caps;">Input Device</span> -->
        </p>

        <style>
            /* Ensure the images share the container width equally, and are centered vertically */
            .image-container {
                display: flex;
                align-items: center; /* Centers items vertically in the container */
            }
            .image-container figure {
                width: 50%;
                margin: 0 5px 0 0; /* Adjust space between the figures if needed */
                text-align: center;
            }
            .image-container img {
                width: 100%; /* Makes the image fill the figure element */
                display: block; /* Removes the bottom margin/space under the image */
            }
            /* Reset the last figure margin if you added margin between them */
            .image-container figure:last-child {
                margin-right: 0;
            }
        </style>
    

        <div class="image-container">
            <figure>
                <img src="images/topicStat.pdf" alt="Image 1">
                <figcaption>Categories of entities in EUREQA</figcaption>
            </figure>
            <figure>
                <img src="images/stat.png" alt="Image 2">
                <figcaption>Splits of questions in EUREQA.</figcaption>
            </figure>
        </div>
    


</section>


<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="images/bathTub.png"> Performance</h2>
    </div>
  </div>



  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">


  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">

      <p>
        Here we present the accuracy of ChatGPT, Gemini-Pro and GPT-4 on the hard set of EUREQA across different depths d of reasoning (number of layers in the questions). We evaluate two prompt strategies: <b>direct</b> zero-shot prompt and <b>ICL</b> with two examples.
        In general, with the entities recursively substituted by the descriptions of reasoning chaining layers, and therefore eliminating surface-level semantic cues, these models generate more incorrect answers. When the reasoning depth increases from one to five on hard questions, there is a notable decline in performance
        for all models. This finding underscores the significant impact that semantic shortcuts have on the accuracy of responses, and it also indicates that GPT-4 is considerably more capable of identifying and taking advantage of these shortcuts.      
      </p>

    </div>
  </div>

  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
        <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">  
            <style>
                table {
                    width: 100%;
                    border-collapse: collapse;
                }
                th, td {
                    border: 1px solid black;
                    text-align: center;
                    padding: 8px;
                    vertical-align: middle; /* Centers content vertically */
                }
                /* th {
                    background-color: #f2f2f2;
                } */
                .header-cell {
                    background-color: #f2f2f2;
                    text-align: center; /* Ensures text is centered horizontally */
                    vertical-align: middle; /* Ensures text is centered vertically */
                }
            </style>
<table>
    <!-- <tr>
        <td class="header-cell" colspan="1"></td>
        <th class="header-cell" colspan="10">hard</th> -->
        <!-- <th class="header-cell" colspan="2">easy</th> -->
    <!-- </tr> -->
    <tr>
        <td>depth</td>
        <td class="header-cell" colspan="2">d=1</td>
        <td class="header-cell" colspan="2">d=2</td>
        <td class="header-cell" colspan="2">d=3</td>
        <td class="header-cell" colspan="2">d=4</td>
        <td class="header-cell" colspan="2">d=5</td>
        <!-- <td class="header-cell" colspan="2">d=5</td> -->
    </tr>
    <tr>
        <td></td>
        <td>direct</td>
        <td>icl</td>
        <td>direct</td>
        <td>icl</td>
        <td>direct</td>
        <td>icl</td>
        <td>direct</td>
        <td>icl</td>
        <td>direct</td>
        <td>icl</td>
        <!-- <td>direct</td>
        <td>icl</td> -->
    </tr>
    <tr>
        <td>ChatGPT</td>
        <td>22.3</td>
        <td>53.3</td>
        <td>7.0</td>
        <td>40.0</td>
        <td>5.0</td>
        <td>39.2</td>
        <td>3.7</td>
        <td>39.3</td>
        <td>7.2</td>
        <td>39.0</td>
        <!-- <td>13.1</td>
        <td>47.0</td> -->
    </tr>
    <tr>
        <td>Gemini-Pro</td>
        <td>45.0</td>
        <td>49.3</td>
        <td>29.5</td>
        <td>23.5</td>
        <td>27.3</td>
        <td>28.6</td>
        <td>25.7</td>
        <td>24.3</td>
        <td>17.2</td>
        <td>21.5</td>
        <!-- <td>30.6</td>
        <td>38.9</td> -->
    </tr>
    <tr>
        <td>GPT-4</td>
        <td>60.3</td>
        <td>76.0</td>
        <td>50.0</td>
        <td>63.7</td>
        <td>51.3</td>
        <td>61.7</td>
        <td>52.7</td>
        <td>63.7</td>
        <td>46.9</td>
        <td>61.9</td>
        <!-- <td>66.4</td>
        <td>81.8</td> -->
    </tr>
</table>


        </div>
    </div>
  </div>
</section>




<section class="section">
    <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
            <h2 class="title is-3"><img width="5%" src="images/shushu.png"> Analyses and discussion</h2>
        </div>
      </div>



<div class="container is-max-desktop">

  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <!-- <h2 class="title is-3"><img width="5%" src="images/shushu.png"> Analyses and discussion</h2> -->

      <h2 class="title is-4">Can human solve EUREQA?</h2>
      <p>
        We carried out a human analysis focusing on the hard set characterized by a reasoning depth of five. This evaluation involves two computer science PhD students as the annotators to ensure their expertise. A selection of 50 questions, randomly extracted from the set, was subjected to annotation. The averaged accuracy of annotators achieves 95% with an Inter-annotator Agreement of Cohen κ = 0.79.
      </p>


      <h2 class="title is-4">Do LLMs take Shortcuts?</h2>
      <p>
        One of our main motivations is to test if language models can follow a simple yet effective reasoning chain instead of taking semantic shortcuts based on entity associations. We design an analytical experiment based on entity similarity to investigate whether LLMs take such semantic shortcuts.
      </p>


      <h2 class="title is-4"> Do open source LLMs perform better?</h2>
      <p>
        To expand the scope of our findings, we experimented with open-sourced LLAMA-2 models across different sizes on hard questions of EUREQA. Similar to our observations on GPT-series models, there’s a notable decline in the accuracy of Llama models as the reasoning depth increases from one to five on the hard set. 
      </p>


      <h2 class="title is-4"> Will prompting solve EUREQA?</h2>
      <p>
        Although the effectiveness of prompting techniques is out of the scope of this paper, we still additionally tested the Tree of Thought(TOT) <a href="https://arxiv.org/pdf/2305.10601.pdf">(Yao et al.,2023)</a> method on ChatGPT with a “propose” strategy, which tries to decompose the questions layer by layer and solve them sequentially. Our results
        show that such a prompting method fails completely even if we provide human-written examples for question decomposition. In no experiment did the TOT method generate a valid answer in the final response.
        
      </p>


      <h2 class="title is-4"><b>Will optimal retrieval solve EUREQA?</b></h2>
      <p>
        Although our knowledge filtering process has already removed the knowledge barrier, it can still be pointed out that whether Retrieval Augmented Generation(RAG) method can solve this task. To address such concerns, we tested GPT-4 on 300 randomly sampled 5-layer hard questions. To minimize the impact of the retrieval method, we investigate a “performance upper bound” setting, which directly injects the retrieval result of the visible entities and relations in the input question from DBpedia. More detailed explanation can be found in the paper. The accuracy of GPT-4 through the above process is 62.0%, which is close to our reported performance of 61.9% in the original setting. We can
        then hypothesize that simply injecting knowledge into the model can hardly solve the problem and the bottleneck remains at the reasoning/question decomposition ability of LLMs. Moreover, it can
        be concluded that our knowledge-filtering process can effectively estimate parametric knowledge, and it is also reliable after self-consistency.
      </p>

      <section class="section"  >
        <div class="container is-max-desktop" id="fig1">
    
          <figure style="text-align: center;">
            <img id="teaser" width="100%" src="images/fig2.pdf">  
            <figcaption>
                Even given all the required information needed for the question (selected information shown in the figure),
                GPT-4 still makes mistakes starting early layers (highlighted in grey). We only show partial output here. Notice that
                we give GPT few-shot prompts.
            </figcaption>
          </figure>
    <!--  -->
        </div>
      </section>



    </div>
</div>
</div>
    




</section>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  @article{li2024eureqa,
      title={Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?}, 
      author={},
      year={2024},
      eprint={2311.09702},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
  </code></pre>
    </div>
  </section> -->
  
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, <a href="https://universal-ner.github.io/">UniversalNER</a> and <a href="https://llava-vl.github.io/">LLaVA</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.  We thank the LLaMA team for giving us access to their models.
      </p>

      <p>
<b>Usage and License Notices</b>: The data abd code is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, ChatGPT, and the original dataset used in the benchmark. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.
</p>
    </div>
  </section>

</body>

</html>
